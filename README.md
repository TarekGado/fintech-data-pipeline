# Fintech Data Pipeline with Apache Airflow, PostgreSQL, and Superset

## Overview

This project focuses on building a data pipeline for fintech data, involving data cleaning, transformation, and visualization. The pipeline processes raw data using Python, Apache Airflow, PostgreSQL, and Superset. The pipeline automates data cleaning, transformation, and visualization.

## Workflow

1. **Data Cleaning & Preprocessing**  
   - Implemented a Python script to clean, transform, and prepare the data.  
   - Tasks included handling missing values and outliers, standardizing formats, and ensuring data consistency.  

2. **ETL Pipeline with Apache Airflow**  
   - Airflow DAGs orchestrate the data pipeline.  
   - The pipeline extracts raw data, processes it, and loads it into PostgreSQL.  

3. **Database Storage**  
   - Cleaned data is stored in PostgreSQL for structured querying.  

4. **Data Visualization with Apache Superset**  
   - Superset dashboards provide insights and trends from the processed data.  

## Technologies Used

- **Python for data processing**
- **Apache Airflow for workflow automation**
- **PostgreSQL for data storage**
- **Apache Superset for visualization**
- **Docker for containerized deployment**

